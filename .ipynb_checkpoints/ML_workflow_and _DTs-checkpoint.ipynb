{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6adbc7b",
   "metadata": {},
   "source": [
    "# ML workflow and decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e65b6",
   "metadata": {},
   "source": [
    "### by: Anh Thu Doan and Amr Mohamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b640f",
   "metadata": {},
   "source": [
    "# Data exploration and preparation to be used for training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9fd728",
   "metadata": {
    "id": "df9fd728"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt   \n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98e9d8d",
   "metadata": {
    "id": "a98e9d8d"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file.\n",
    "data = pd.read_csv('CTG.csv', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3b3abc",
   "metadata": {
    "id": "0a3b3abc"
   },
   "outputs": [],
   "source": [
    "# Select the relevant numerical columns.\n",
    "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
    "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
    "                 'Median', 'Variance', 'Tendency', 'NSP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10eace4d",
   "metadata": {
    "id": "10eace4d"
   },
   "outputs": [],
   "source": [
    "data = data[selected_cols].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b66f552",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "3b66f552",
    "outputId": "28954686-e87b-4c66-d702-204ed0cbe3b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>DP</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>...</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LB   AC   FM   UC   DL   DS   DP  ASTV  MSTV  ALTV  ...   Min    Max  \\\n",
       "0  120.0  0.0  0.0  0.0  0.0  0.0  0.0  73.0   0.5  43.0  ...  62.0  126.0   \n",
       "1  132.0  4.0  0.0  4.0  2.0  0.0  0.0  17.0   2.1   0.0  ...  68.0  198.0   \n",
       "2  133.0  2.0  0.0  5.0  2.0  0.0  0.0  16.0   2.1   0.0  ...  68.0  198.0   \n",
       "3  134.0  2.0  0.0  6.0  2.0  0.0  0.0  16.0   2.4   0.0  ...  53.0  170.0   \n",
       "4  132.0  4.0  0.0  5.0  0.0  0.0  0.0  16.0   2.4   0.0  ...  53.0  170.0   \n",
       "\n",
       "   Nmax  Nzeros   Mode   Mean  Median  Variance  Tendency  NSP  \n",
       "0   2.0     0.0  120.0  137.0   121.0      73.0       1.0  2.0  \n",
       "1   6.0     1.0  141.0  136.0   140.0      12.0       0.0  1.0  \n",
       "2   5.0     1.0  141.0  135.0   138.0      13.0       0.0  1.0  \n",
       "3  11.0     0.0  137.0  134.0   137.0      13.0       1.0  1.0  \n",
       "4   9.0     0.0  137.0  136.0   138.0      11.0       1.0  1.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0648208c",
   "metadata": {
    "id": "0648208c"
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "data_shuffled = data.sample(frac=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28cb83be",
   "metadata": {
    "id": "28cb83be"
   },
   "outputs": [],
   "source": [
    "# Split into input part X and output part Y.\n",
    "X = data_shuffled.drop('NSP', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6370b955",
   "metadata": {
    "id": "6370b955"
   },
   "outputs": [],
   "source": [
    "# Map the diagnosis code to a human-readable label.\n",
    "def to_label(y):\n",
    "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc63a48b",
   "metadata": {
    "id": "dc63a48b"
   },
   "outputs": [],
   "source": [
    "Y = data_shuffled['NSP'].apply(to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2444ab8",
   "metadata": {
    "id": "c2444ab8"
   },
   "outputs": [],
   "source": [
    "# Partition the data into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980574ca",
   "metadata": {},
   "source": [
    "# Baseline classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53d27d",
   "metadata": {
    "id": "de53d27d"
   },
   "source": [
    "# Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b967cf39",
   "metadata": {
    "id": "b967cf39"
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d403923",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d403923",
    "outputId": "e8fc0245-d431-443c-c508-1dd67c1462c7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7805882352941176"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(dummy, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a625604",
   "metadata": {},
   "source": [
    "By cross-validating the dummy classifier on the training data's different fold, we get a mean accuracy of <b>78% </b> in predicting the classification of the fetal states into the three class Normal, Suspicious, or Pathological."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362216b",
   "metadata": {},
   "source": [
    "## Training and hyperparameter-tuning a set of different ML classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbe024",
   "metadata": {},
   "source": [
    "In this section we are going to expirement few different types of classifiers for the aim of finding the classifier with the highest accuracy in the differentiation task of the fetal states classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d62b88",
   "metadata": {
    "id": "97d62b88"
   },
   "source": [
    "# Tree-based classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725955f",
   "metadata": {},
   "source": [
    "We firstly start by experimenting the Decision Tree classifier of sklearn on our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90f924",
   "metadata": {
    "id": "ea90f924"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4496f23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4496f23",
    "outputId": "90e628c4-f8c0-470a-c10e-9019390fb589"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205882352941176"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "skDT = DecisionTreeClassifier()\n",
    "cross_val_score(skDT, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388cf2ed",
   "metadata": {},
   "source": [
    "As we can see, the DT classifier obtained a cross-validation accuracy of an average of <b>92.4%</b> on the different folds of the training data when being called with the default values of its parameters, so we proceed by tuning the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030b8e2",
   "metadata": {
    "id": "0030b8e2"
   },
   "source": [
    "### DecisionTree hyperparameter (max_depth) tuning through a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e99331",
   "metadata": {
    "id": "d9e99331"
   },
   "source": [
    "plot to verify the determine the max_depth hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fad3c59f",
   "metadata": {
    "id": "fad3c59f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt   \n",
    "from matplotlib.legend_handler import HandlerLine2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92cd58c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "92cd58c8",
    "outputId": "841b8b1a-c057-42fc-84ed-2e09984f1c4f",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_depths = np.linspace(1, 50, 50, endpoint=True)\n",
    "\n",
    "train_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth,random_state=0)\n",
    "    accuracyTemp = cross_val_score(dt, Xtrain, Ytrain).mean()\n",
    "    train_results.append(accuracyTemp)\n",
    "    \n",
    "line2 = plt.plot(max_depths, train_results, label='Train accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5706ef3",
   "metadata": {},
   "source": [
    "As we can see in the graph, the maximum depth maximizes the accuracy of the DT classifier when its maximimum depth is between 8-10 approximately and decreases to stay constant after the depth of 18. (This cv accuracy values are guaranteed when the max_depth is varying while the other parameters are held with their default values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46208e1e",
   "metadata": {
    "id": "46208e1e"
   },
   "source": [
    "### DecisionTree hyperparameter tuning using sklearn gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2feaa",
   "metadata": {},
   "source": [
    "Moving on, to tune the hyperparameters max_depth and criterion with different combinations of their values in order to find the best combination that maximizes the model's csv average accuracy, we use sklearn's gridsearch meathod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcea0ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcea0ab7",
    "outputId": "9c2a6f6f-9147-4ce3-b795-b0ed7e2e38e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [3, 5, 6, 7, 8, 9, 10, 40, 70, 100]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dTgridTuned = DecisionTreeClassifier(random_state=0)\n",
    "parameter_space = {'max_depth': [3,5,6,7,8,9,10,40,70,100],\n",
    "                'criterion': ['gini','entropy']}\n",
    "DtTuned = GridSearchCV(dTgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "DtTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af814dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4af814dc",
    "outputId": "52d17261-e238-4867-8b84-9e655a6afa9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 40}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DtTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496487d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "496487d3",
    "outputId": "d32f09e1-5a6d-476e-f719-7a79ef66edb4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9341176470588234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DtTuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b26160",
   "metadata": {},
   "source": [
    "By running searching the search space of the different combination of both parameters, the gridsearch found that when <b> criterion = 'entropy'</b> and <b> max_depth = 40</b>, the cv accuracy is maximized to <b>93.4% </b> in predicting the status of the fetus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0462e16",
   "metadata": {
    "id": "b0462e16"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5b0d0",
   "metadata": {},
   "source": [
    "Afterwards, we moved forward to experiment the Random Forest classifier model, since the Decision tree classifier performed well compared to the baseline dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dba30d2c",
   "metadata": {
    "id": "dba30d2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9429411764705883"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(random_state=0)\n",
    "cross_val_score(RF, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2d5ae",
   "metadata": {},
   "source": [
    "As we can see, the RF classifier obtained a cross-validation accuracy of an average of <b>94%</b> on the different folds of the training data when being called with the default values of its parameters, so we proceed by tuning the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0225017",
   "metadata": {
    "id": "d0225017"
   },
   "source": [
    "### n_estimators hyperparameter tuning with graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f458409",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5f458409",
    "outputId": "ebdd9b86-ea23-4781-f910-2c6554b3ce1f",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200,500]\n",
    "train_results = []\n",
    "for estimator in n_estimators:\n",
    "    RF = RandomForestClassifier(n_estimators=estimator,random_state=0)\n",
    "    accuracyTemp = cross_val_score(RF, Xtrain, Ytrain).mean()\n",
    "    train_results.append(accuracyTemp)\n",
    "    \n",
    "plt.plot(n_estimators, train_results, label='Train accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447e42d",
   "metadata": {},
   "source": [
    "As we can see in the graph above, the number of estimator trees that maximizes the accuracy of the RF classifier when its n_estimators is 120 estimators approximately and the accuracy starts in decreasing slightly as the number of estimators increases. (This cv accuracy values are guaranteed when the n_estimators is varying while the other parameters are held with their default values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce57757",
   "metadata": {
    "id": "fce57757"
   },
   "source": [
    "### Random Forest Classifier hyperparameter tuning with gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c852475",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c852475",
    "outputId": "e90266aa-2cf7-4563-e018-bac349f4e787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [0, 2, 3, 5, 10, 40, 70, 100],\n",
       "                         'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200,\n",
       "                                          500]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RfCgridTuned = RandomForestClassifier()\n",
    "parameter_space = {'max_depth': [0,2,3,5,10,40,70,100],\n",
    "                 'criterion': ['gini', 'entropy'],\n",
    "                   'n_estimators' : [1, 2, 4, 8, 16, 32, 64, 100, 200,500]}\n",
    "RFCTuned = GridSearchCV(RfCgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "RFCTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1fbbbdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1fbbbdb",
    "outputId": "4806a19b-c374-42ba-c39a-4cb0706d6dc5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 40, 'n_estimators': 32}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFCTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2105a151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2105a151",
    "outputId": "2edfdd8e-2d36-4425-955a-236838c649c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9447058823529412"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFCTuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee968be",
   "metadata": {},
   "source": [
    "As we can see, for the purpose of reaching a higher accuracy in the cross-validation phase, we tune the hyperparameter <i>maxdepth, criterion, and n_estimators </i> of the model using a gridsearch to find the the parameters that maximized the model's cv accuracy are <b>criterion = 'entropy', max_depth = 10, and n_estimators = 32</b> as by applying those parameters values on the model, the model acieved a cv accuracy of <b>94.4%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41d00e",
   "metadata": {
    "id": "db41d00e"
   },
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217748d",
   "metadata": {},
   "source": [
    "We continue to repeat the steps carried out on the models above for more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c67c35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0c67c35",
    "outputId": "10f09e05-e509-4873-8ce4-49cebb4a0813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9494117647058824"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state=0)\n",
    "cross_val_score(GBC, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beebb52",
   "metadata": {},
   "source": [
    "We experiment the Gradient Boosting Classifier model on the training data, and we find that with the model's default parameters, the model achived an average cross validation accuracy on the different folds of the training set of approximately <b> 95% </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe6cbc",
   "metadata": {
    "id": "97fe6cbc"
   },
   "source": [
    "### max_depth hyperparameter tuning through a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f3ed9",
   "metadata": {
    "id": "0f7f3ed9"
   },
   "source": [
    "plot to verify the determine the max_depth hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f63aef8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "f63aef8d",
    "outputId": "9d1d60dd-dd9f-4b9d-e6d2-d202c3ecf329",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_depths = np.linspace(1, 10, 10, endpoint=True)\n",
    "\n",
    "train_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    GBC = GradientBoostingClassifier(max_depth=max_depth,random_state=0)\n",
    "    accuracyTemp = cross_val_score(GBC, Xtrain, Ytrain).mean()\n",
    "    train_results.append(accuracyTemp)\n",
    "    \n",
    "plt.plot(max_depths, train_results, label='Train accuracy')\n",
    "    \n",
    "# plt.ylim(bottom=0)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d1b24",
   "metadata": {},
   "source": [
    "By tuning the model's hyperparameter <b> max_depth</b>, we can see from the graph aove that the value of <b> max_depth</b> that maximizes the models average cross validation accuracy on the different fold of the training set is <b> max_depth = 6</b> that results in an avergae cv accuracy of aprroximately <b>95.2%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d9312",
   "metadata": {
    "id": "a90d9312"
   },
   "source": [
    "### GradientBoosting Classifier hyperparameter tuning with gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8332799e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8332799e",
    "outputId": "e60ab9e0-ea8f-4204-e78a-43977bd139b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['friedman_mse', 'squared_error', 'mse',\n",
       "                                       'mae'],\n",
       "                         'learning_rate': [0.001, 0.01, 1],\n",
       "                         'loss': ['deviance', 'exponential'],\n",
       "                         'n_estimators': [4, 8, 16, 64, 100]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBCgridTuned = GradientBoostingClassifier()\n",
    "parameter_space = {'loss' : ['deviance', 'exponential'],\n",
    "                'criterion': ['friedman_mse', 'squared_error', 'mse', 'mae'],\n",
    "                'n_estimators' : [4, 8, 16, 64, 100]\n",
    "                ,'learning_rate': [0.001,0.01,1]\n",
    "                  }\n",
    "GBCTuned = GridSearchCV(GBCgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "GBCTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e50cd73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e50cd73",
    "outputId": "86299249-4e23-47c9-80de-ff96b25218d0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'squared_error',\n",
       " 'learning_rate': 1,\n",
       " 'loss': 'deviance',\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBCTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c4fc2ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c4fc2ba",
    "outputId": "188f7865-bbe7-4ba4-82b8-080881ace265",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9441176470588235"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBCTuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8acd6",
   "metadata": {},
   "source": [
    "furthermore, we tuned the differen hyperparameters of the model <b> loss, criterion, n_estimators, and learning_rate </b> using a gridsearch to get a best score of the different combination of the gridsearch of <b> 94.2%</b>, which is less than the average cross validation accuracy obtained by max_depth=6 with the rest of the parameters set to their defaults, therefore for the GradientBoosting Classifier model we use the graphically tuned version with <b>max_depth = 6</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "mHwVqP2gafAY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHwVqP2gafAY",
    "outputId": "7aaaac12-6472-45b6-c309-ff9523499d2d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523529411764706"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBCFinal = GradientBoostingClassifier(max_depth = 6)\n",
    "cross_val_score(GBCFinal, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167eda91",
   "metadata": {
    "id": "167eda91"
   },
   "source": [
    "# Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a4ce6",
   "metadata": {},
   "source": [
    "Afterwards, we moved forward to try a set of different linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c3072",
   "metadata": {
    "id": "645c3072"
   },
   "source": [
    "## Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b01ee624",
   "metadata": {
    "id": "b01ee624"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "537ef959",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "537ef959",
    "outputId": "034232e4-17ab-427e-9925-659e225aecf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825294117647059"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "cross_val_score(perceptron, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065050f",
   "metadata": {},
   "source": [
    "For perceptron linear model, we obtaind an average cross validation accuracy on the different folds of the training set of <b>82.5%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a07bce",
   "metadata": {
    "id": "c8a07bce"
   },
   "source": [
    "### Hyperparameter tuning with sklearn grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0119c9c7",
   "metadata": {
    "id": "0119c9c7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06bb6a",
   "metadata": {
    "id": "da06bb6a"
   },
   "source": [
    "we tune the perceptron model based on the learning rate (alpha), penalty, and number of epochs as they are the most parameters having effect on the model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdc08700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdc08700",
    "outputId": "d6105438-d037-46df-cc7c-41d35021cb49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Perceptron(), n_jobs=-1,\n",
       "             param_grid={'alpha': [1e-05, 0.0001, 0.0003, 0.001, 0.003, 0.01,\n",
       "                                   0.03, 0.1, 0.3],\n",
       "                         'max_iter': [3, 5, 10, 15, 20, 50],\n",
       "                         'penalty': ['l2', 'l1', 'elasticnet']})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron(random_state=0)\n",
    "parameter_space = {'alpha': [0.00001,0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n",
    "                   'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                'max_iter': [3,5, 10, 15, 20, 50]}\n",
    "perceptronTuned = GridSearchCV(perceptron, parameter_space, n_jobs=-1, cv=5)\n",
    "perceptronTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99f65b1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99f65b1e",
    "outputId": "3eb6bc44-3799-4a0b-b351-a705ef31a24d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001, 'max_iter': 50, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptronTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61b2714a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61b2714a",
    "outputId": "3ee9f4e6-b044-4f2d-dbea-6fce3229af4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8505882352941176"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptronTuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cb600",
   "metadata": {},
   "source": [
    "By tuning the hyperparameters of the perceptron model using a grid search, the average cv accuracy on the different fold of the training set is raised to <b>85%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5064cda",
   "metadata": {
    "id": "a5064cda"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f387b36",
   "metadata": {
    "id": "3f387b36"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9be87666",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9be87666",
    "outputId": "88bb5605-03a5-403b-e6e7-1dba54bdeba0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8729411764705881"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogR = LogisticRegression()\n",
    "cross_val_score(LogR, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc5294b",
   "metadata": {},
   "source": [
    "For the Logistic Regression model, we obtaind an average cross validation accuracy on the different folds of the training set of <b>87.4%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8223ded",
   "metadata": {
    "id": "b8223ded"
   },
   "source": [
    "### Tuning logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcb7390d",
   "metadata": {
    "id": "bcb7390d"
   },
   "outputs": [],
   "source": [
    "LogRgridTuned = LogisticRegression()\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100,1000], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "LogRTuned = GridSearchCV(LogRgridTuned, param_grid)\n",
    "\n",
    "LogRTuned.fit(Xtrain, Ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "945710ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "945710ff",
    "outputId": "544ef3c1-90d8-4f3a-bd30-18220df37401"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "187caec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "187caec5",
    "outputId": "81c226d0-3b82-47ef-ddf2-11e9ff54835e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8811764705882353"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRTuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14dcfc",
   "metadata": {},
   "source": [
    "By tuning the logistic regression model on the <b> Inverse of regularization strength (C) and the norm of the penalty </b>, we find that the hyperparameter combination that maximizes the average cv accuracy to approximately <b> 88%</b> is <b> C = 0.01 and penalty = L2</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf77a7",
   "metadata": {
    "id": "0aaf77a7"
   },
   "source": [
    "## Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63bbb589",
   "metadata": {
    "id": "63bbb589"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56358b42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56358b42",
    "outputId": "7b727b03-9b4d-48f4-fc98-33742f31554a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452941176470589"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(random_state=0)\n",
    "cross_val_score(lsvc, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d7c1f",
   "metadata": {},
   "source": [
    "For the Linear Support Vector Classifier model, we obtaind an average cross validation accuracy on the different folds of the training set of <b> 84.5%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a6c0f",
   "metadata": {
    "id": "985a6c0f"
   },
   "source": [
    "### Linear SVC hyperparameter tuning with gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7d74eb7",
   "metadata": {
    "id": "f7d74eb7"
   },
   "outputs": [],
   "source": [
    "lsvcGridTuned = LinearSVC()\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100,1000], 'penalty': ['l1', 'l2'],'loss':['hinge', 'squared_hinge']}\n",
    "\n",
    "lsvcTuned = GridSearchCV(lsvcGridTuned, param_grid)\n",
    "\n",
    "lsvcTuned.fit(Xtrain, Ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0909f035",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0909f035",
    "outputId": "6b57dd7c-e25d-4c1d-84ab-b0f6041fea5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'loss': 'hinge', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvcTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f457495f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f457495f",
    "outputId": "098e0a8c-898f-419e-8add-b5d99369ae04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8688235294117647"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvcTuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924b252",
   "metadata": {},
   "source": [
    "By tuning the Linear SVC model on the <b> the Regularization parameter (C), the loss function, and the norm of the penalty </b>, we find that the hyperparameter combination that maximizes the average cv accuracy to approximately <b> 88%</b> is <b> C = 100, loss = hinge,  and penalty = L2</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ee385",
   "metadata": {
    "id": "749ee385"
   },
   "source": [
    "# Neural network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc2af1",
   "metadata": {},
   "source": [
    "Moreover, we experiment the neural network based model Multilayer perceptron from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IOnFRpdMwGRS",
   "metadata": {
    "id": "IOnFRpdMwGRS"
   },
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7c5091f",
   "metadata": {
    "id": "a7c5091f"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db704822",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db704822",
    "outputId": "af86778f-081f-43b6-a9f7-36e4db207fad",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8847058823529412"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NNC = MLPClassifier(random_state=0)\n",
    "cross_val_score(NNC, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de310d",
   "metadata": {},
   "source": [
    "For the MLP Classifier model, we obtaind an average cross validation accuracy on the different folds of the training set of <b>88.5%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214b546",
   "metadata": {
    "id": "9214b546"
   },
   "source": [
    "### Hyperparameter tuning for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "GZVFwpKozAc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZVFwpKozAc3",
    "outputId": "491183ff-3db4-4b7c-cd2f-b9b47866082a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_gs = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(1000,),(5000,)],\n",
    "    'activation': ['tanh', 'relu','logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001],\n",
    "    'learning_rate': ['invscaling', 'adaptive'],\n",
    "}\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2rkNpdz0zLo6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rkNpdz0zLo6",
    "outputId": "09a3322e-e022-41c3-d11f-c9174d7971a9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba75470",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ead14",
   "metadata": {},
   "source": [
    "By tuning the MLP classifier model on the parameter <b> number of hidden layers, activation function, solver, learning rate, and type of learning rate </b>, we obtained that the values of the parameters specified above resulted in an increase in the average cv accuracy to approximately <b> 91.2%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ed689",
   "metadata": {
    "id": "365ed689"
   },
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a446fb8",
   "metadata": {},
   "source": [
    "Moving forward, we choose the highest models scoring average cross validation accuracies to predict with them the testing set and evaluate their prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e8ae9",
   "metadata": {
    "id": "a48e8ae9"
   },
   "source": [
    "### Cross Validation Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51816b47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51816b47",
    "outputId": "134eed2a-dd08-42a1-e1cb-a8a9080f7b5b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dummy Classifier\n",
    "print(\"The cross-validation accuarcy of the Dummy Classifier is :\"+str(round(cross_val_score(dummy, Xtrain, Ytrain).mean(),3)))\n",
    "# Decision Tree\n",
    "print(\"The cross-validation accuarcy of the Decision Tree Classifier is :\"+str(round(DtTuned.best_score_,3)))\n",
    "# Random Forset\n",
    "print(\"The cross-validation accuarcy of the Random Forest Classifier is :\"+str(round(RFCTuned.best_score_,3)))\n",
    "#Gradient Boosting\n",
    "print(\"The cross-validation accuarcy of the Gradient Boosting Classifier is :\"+str(round(cross_val_score(GBCFinal, Xtrain, Ytrain).mean(),3)))\n",
    "#Perceptron\n",
    "print(\"The cross-validation accuarcy of the Perceptron Classifier is :\"+str(round(perceptronTuned.best_score_,3)))\n",
    "# Logistic Regression\n",
    "print(\"The cross-validation accuarcy of the Logistic Regression Classifier is :\"+str(round(perceptronTuned.best_score_,3)))\n",
    "# LinearSVC\n",
    "print(\"The cross-validation accuarcy of the Linear support vector classifier is :\"+str(round(lsvcTuned.best_score_,3)))\n",
    "# Multi layer perceptron (MLP) Neural Network Classifier\n",
    "print(\"The cross-validation accuarcy of the Multi layer perceptron (MLP) Neural Network Classifier is :\"+str(round(clf.best_score_,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d16a4f",
   "metadata": {
    "id": "e5d16a4f"
   },
   "source": [
    "We can see that the Gradient Boosting Classifier performed the best within the used classifiers in classification of the <b>  fetal states </b> into the three class <i>Normal<i>, <i>Suspicious<i>, or <i>Pathological<i> with a average validation accuracy of <b>95%</b> with a difference of 0.4% average cross-validation accuracy than the validation accuracy of the Random Forest Tree classifier, and 1.5% difference than the Decision Tree classifier, we proceed forward to test the prediction accuracy of those 3 models on the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085fa38",
   "metadata": {
    "id": "6085fa38"
   },
   "source": [
    "## Test data prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023108e",
   "metadata": {
    "id": "2023108e"
   },
   "source": [
    "###  Gradient Boosting Classifier predicitions and accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f40fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "453f40fd",
    "outputId": "2c77078f-8cd4-4340-d523-fe2bdb37a0ab"
   },
   "outputs": [],
   "source": [
    "GBCFinal.fit(Xtrain, Ytrain)\n",
    "Yguess = GBCFinal.predict(Xtest)\n",
    "print('First 10 tuples of test-predicted data (Actual value, predicted value)')\n",
    "cpt=0\n",
    "for i in zip(Ytest,Yguess):\n",
    "    cpt+=1\n",
    "    if cpt <11:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6f0e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87c6f0e3",
    "outputId": "8fbc0137-5e03-40e0-db1b-3767a908deff"
   },
   "outputs": [],
   "source": [
    "print('Testing accuracy of Gradient Boosting: '+str(round(accuracy_score(Ytest, Yguess),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zJP-71AR9c4Q",
   "metadata": {
    "id": "zJP-71AR9c4Q"
   },
   "source": [
    "###  Random Forest Classifier predicitions and accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mrJU7zfl9X4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mrJU7zfl9X4f",
    "outputId": "c3114843-e4b0-4f41-e8df-ccb0db09d4d3"
   },
   "outputs": [],
   "source": [
    "Yguess = RFCTuned.predict(Xtest)\n",
    "print('First tuples 10 of test-predicted data (Actual value, predicted value)')\n",
    "cpt=0\n",
    "for i in zip(Ytest,Yguess):\n",
    "    cpt+=1\n",
    "    if cpt <11:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8juyD1x890Xq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8juyD1x890Xq",
    "outputId": "58b6d498-7514-4e03-bc00-308444dcee37"
   },
   "outputs": [],
   "source": [
    "print('Testing accuracy of Random Forest: '+str(round( accuracy_score(Ytest, Yguess),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XfmnhhLF-KJN",
   "metadata": {
    "id": "XfmnhhLF-KJN"
   },
   "source": [
    "###  Decision Tree classifier predicitions and accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1Is3Nh-_-Ni9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Is3Nh-_-Ni9",
    "outputId": "b00a07a0-fcbd-4308-f8fc-83f21b2a2a73"
   },
   "outputs": [],
   "source": [
    "Yguess = DtTuned.predict(Xtest)\n",
    "print('First 10 tuples of test-predicted data (Actual value, predicted value)')\n",
    "cpt=0\n",
    "for i in zip(Ytest,Yguess):\n",
    "    cpt+=1\n",
    "    if cpt <11:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-R2zZ3oE-QvV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-R2zZ3oE-QvV",
    "outputId": "87a787a4-b37f-4873-8c2d-6c8900d857c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Testing accuracy of Decision Tree: '+str(round( accuracy_score(Ytest, Yguess),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3df91",
   "metadata": {},
   "source": [
    "From the classifiers above, we can see that the highest scoring classifier is the Random Forest Classifier with a prediction accuracy of 93.7% accuracy on the test set, and a gap of 0.3% between the cv accuracy and the testing accuracy, therefore we know that the classifier is not overfitting the training data, and can be used for future predictions. Recall that a gridsearch was used to tune the hyperparameters of this model to find the values of the hyperparameters as <b>'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 32</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6a9fe",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1b992",
   "metadata": {
    "id": "8d38c2f2"
   },
   "source": [
    "# Implementing a Decision Tree class for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc168424",
   "metadata": {
    "id": "58161c03"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeLeaf:\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
    "    def predict(self, x):\n",
    "        return self.value\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_id = str(node_counter)\n",
    "        val_str = f'{self.value:.4g}' if isinstance(self.value, float) else str(self.value)\n",
    "        graph.node(node_id, val_str, style='filled')\n",
    "        return node_counter+1, node_id\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, DecisionTreeLeaf):\n",
    "            return self.value == other.value\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3bd19",
   "metadata": {
    "id": "4864028c"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeBranch:\n",
    "\n",
    "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.low_subtree = low_subtree\n",
    "        self.high_subtree = high_subtree\n",
    "\n",
    "    # For a branch node, we compute the prediction by first considering the feature, and then \n",
    "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
    "    # than the threshold.\n",
    "    def predict(self, x):\n",
    "        if x[self.feature] <= self.threshold:\n",
    "            return self.low_subtree.predict(x)\n",
    "        else:\n",
    "            return self.high_subtree.predict(x)\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_counter, low_id = self.low_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_counter, high_id = self.high_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_id = str(node_counter)\n",
    "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
    "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
    "        graph.node(node_id, lbl, shape='box', fillcolor='yellow', style='filled, rounded')\n",
    "        graph.edge(node_id, low_id, 'False')\n",
    "        graph.edge(node_id, high_id, 'True')\n",
    "        return node_counter+1, node_id\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af38b08",
   "metadata": {
    "id": "d158d182",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DecisionTree(ABC, BaseEstimator):\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
    "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
    "    # called make_tree (see below).\n",
    "    def fit(self, X, Y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.names = X.columns\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, list):\n",
    "            self.names = None\n",
    "            X = np.array(X)\n",
    "        else:\n",
    "            self.names = None\n",
    "        Y = np.array(Y)        \n",
    "        self.root = self.make_tree(X, Y, self.max_depth)\n",
    "        \n",
    "    def draw_tree(self):\n",
    "        graph = Digraph()\n",
    "        self.root.draw_tree(graph, 0, self.names)\n",
    "        return graph\n",
    "    \n",
    "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
    "    # for a set of instances.\n",
    "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        return [self.predict_one(x) for x in X]\n",
    "\n",
    "    # Predicting the output for one instance.\n",
    "    def predict_one(self, x):\n",
    "        return self.root.predict(x)        \n",
    "\n",
    "    # This is the recursive training \n",
    "    def make_tree(self, X, Y, max_depth):\n",
    "\n",
    "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
    "        # For classifiers, this will be the most common value in Y.\n",
    "        default_value = self.get_default_value(Y)\n",
    "\n",
    "        # First the two base cases in the recursion: is the training set completely\n",
    "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
    "\n",
    "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
    "        if max_depth == 0:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # If all the instances in the remaining training set have the same output value,\n",
    "        # return a leaf with this value.\n",
    "        if self.is_homogeneous(Y):\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
    "        # we use one of the classification or regression criteria.\n",
    "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
    "        n_features = X.shape[1]\n",
    "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
    "        # the threshold or not\n",
    "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
    "\n",
    "        # Build the subtrees using a recursive call. Each subtree is associated\n",
    "        # with a value of the feature.\n",
    "        low_subtree = self.make_tree(X_low, Y_low, max_depth-1)\n",
    "        high_subtree = self.make_tree(X_high, Y_high, max_depth-1)\n",
    "\n",
    "        if low_subtree == high_subtree:\n",
    "            return low_subtree\n",
    "\n",
    "        # Return a decision tree branch containing the result.\n",
    "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
    "    \n",
    "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
    "    # and a threshold.\n",
    "    def split_by_feature(self, X, Y, feature, threshold):\n",
    "        low = X[:,feature] <= threshold\n",
    "        high = ~low\n",
    "        return X[low], X[high], Y[low], Y[high]\n",
    "    \n",
    "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_default_value(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_homogeneous(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def best_split(self, X, Y, feature):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18280bf",
   "metadata": {
    "id": "bc4c9ba7"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TreeClassifier(DecisionTree, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, max_depth=10, criterion='maj_sum'):\n",
    "        super().__init__(max_depth)\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # For decision tree classifiers, there are some different ways to measure\n",
    "        # the homogeneity of subsets.\n",
    "        if self.criterion == 'maj_sum':\n",
    "            self.criterion_function = majority_sum_scorer\n",
    "        elif self.criterion == 'info_gain':\n",
    "            self.criterion_function = info_gain_scorer\n",
    "        elif self.criterion == 'gini':\n",
    "            self.criterion_function = gini_scorer\n",
    "        else:\n",
    "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
    "        super().fit(X, Y)\n",
    "        self.classes_ = sorted(set(Y))\n",
    "\n",
    "    # Select a default value that is going to be used if we decide to make a leaf.\n",
    "    # We will select the most common value.\n",
    "    def get_default_value(self, Y):\n",
    "        self.class_distribution = Counter(Y)\n",
    "        return self.class_distribution.most_common(1)[0][0]\n",
    "    \n",
    "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
    "    # this means that all output values are identical.\n",
    "    # We assume that we called get_default_value just before, so that we can access\n",
    "    # the class_distribution attribute. If the class distribution contains just one item,\n",
    "    # this means that the set is homogeneous.\n",
    "    def is_homogeneous(self, Y):\n",
    "        return len(self.class_distribution) == 1\n",
    "        \n",
    "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
    "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
    "    # In the end, we return a triple consisting of\n",
    "    # - the best score we found, according to the criterion we're using\n",
    "    # - the id of the feature\n",
    "    # - the threshold for the best split\n",
    "    def best_split(self, X, Y, feature):\n",
    "\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        sorted_indices = np.argsort(X[:, feature])        \n",
    "        X_sorted = list(X[sorted_indices, feature])\n",
    "        Y_sorted = list(Y[sorted_indices])\n",
    "\n",
    "        n = len(Y)\n",
    "\n",
    "        # The frequency tables corresponding to the parts *before and including*\n",
    "        # and *after* the current element.\n",
    "        low_distr = Counter()\n",
    "        high_distr = Counter(Y)\n",
    "\n",
    "        # Keep track of the best result we've seen so far.\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "        # Go through all the positions (excluding the last position).\n",
    "        for i in range(0, n-1):\n",
    "\n",
    "            # Input and output at the current position.\n",
    "            x_i = X_sorted[i]\n",
    "            y_i = Y_sorted[i]\n",
    "            \n",
    "            # Update the frequency tables.\n",
    "            low_distr[y_i] += 1\n",
    "            high_distr[y_i] -= 1\n",
    "\n",
    "            # If the input is equal to the input at the next position, we will\n",
    "            # not consider a split here.\n",
    "            #x_next = XY[i+1][0]\n",
    "            x_next = X_sorted[i+1]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "\n",
    "            # Compute the homogeneity criterion for a split at this position.\n",
    "            score = self.criterion_function(i+1, low_distr, n-i-1, high_distr)\n",
    "\n",
    "            # If this is the best split, remember it.\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "        # If we didn't find any split (meaning that all inputs are identical), return\n",
    "        # a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "        # Otherwise, return the best split we found and its score.\n",
    "        split_point = 0.5*(X_sorted[max_i] + X_sorted[max_i+1])\n",
    "        return max_score, feature, split_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349e229",
   "metadata": {
    "id": "e24246ee"
   },
   "outputs": [],
   "source": [
    "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
    "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
    "    return maj_sum_low + maj_sum_high\n",
    "    \n",
    "def entropy(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return -sum(p*np.log2(p) if p > 0 else 0 for p in ps)\n",
    "\n",
    "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*entropy(low_distr)+n_high*entropy(high_distr))/(n_low+n_high)\n",
    "\n",
    "def gini_impurity(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return 1-sum(p**2 for p in ps)\n",
    "    \n",
    "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*gini_impurity(low_distr)+n_high*gini_impurity(high_distr))/(n_low+n_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfcd1f",
   "metadata": {},
   "source": [
    "### tree visualization with small max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef62f5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "4341b309",
    "outputId": "ac6e16cc-d9d2-49fc-890f-bbc69f21ca5c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls = TreeClassifier(max_depth=2)\n",
    "cls.fit(Xtrain, Ytrain)\n",
    "cls.draw_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4a3e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2288b61d",
    "outputId": "5d3e4df9-5f5b-48cf-c199-bb0d3a3a9752"
   },
   "outputs": [],
   "source": [
    "Yguess= cls.predict(Xtest)\n",
    "print(accuracy_score(Ytest,Yguess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913941d",
   "metadata": {},
   "source": [
    "In the tree above, we can see that a small max_depth as 2, resulted in a pretty good testing accuracy of 87% eve though it was intended for tree drawing illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a9208",
   "metadata": {
    "id": "20e931ed"
   },
   "source": [
    "# TreeClassifier model max_depth hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265836d",
   "metadata": {
    "id": "bbcbd005",
    "scrolled": true
   },
   "source": [
    "### Tuning using the method using above (max_depth plot vs acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3eeab8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "8adc6655",
    "outputId": "886d831f-c326-4f21-fd11-7dc6253409f9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 100, 100, endpoint=True)\n",
    "\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "    dt = TreeClassifier(max_depth=max_depth)\n",
    "    dt.fit(Xtrain, Ytrain)\n",
    "    Ypred = dt.predict(Xtest)\n",
    "    accuracyTemp = accuracy_score(Ytest, Ypred)\n",
    "\n",
    "    test_results.append(accuracyTemp)\n",
    "line= plt.plot(max_depths, test_results, label='Test accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a0680",
   "metadata": {
    "id": "3e3aba02"
   },
   "source": [
    "by searching the space of [1,100] max_depth we found that max_depth = 7 maximizes the accuracy of the TreeClassifier when the only used parameter is max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e687acc",
   "metadata": {
    "id": "fced6dd7"
   },
   "source": [
    "### Tuning using sklearn gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83ab11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdf2f6c5",
    "outputId": "e14a48c4-200f-484a-939f-f69e9f20f2e0"
   },
   "outputs": [],
   "source": [
    "dTgridTuned = TreeClassifier()\n",
    "parameter_space = {'max_depth': [3,5,7,10,40,70,100],\n",
    "                'criterion': ['gini','maj_sum','info_gain']}\n",
    "DtTuned = GridSearchCV(dTgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "DtTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f0700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f47068e2",
    "outputId": "67679391-cff6-4efa-eb14-810decfba11f"
   },
   "outputs": [],
   "source": [
    "DtTuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706be18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b7c6112",
    "outputId": "23ac6c05-e4dc-4330-ff31-994be29bb802",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DTFinal = TreeClassifier(max_depth=7,criterion='gini')\n",
    "cross_val_score(DTFinal, Xtrain, Ytrain).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6efcb3",
   "metadata": {
    "id": "dcc30266"
   },
   "source": [
    "by tuning the model TreeClassifier with a range of different values for max_depth and different types of splitting criterion, we found that the parameters that maximizes the classifier's accuracy are max_depth=7 and criterion = gini impurity which gives us a cross validation accuracy of 93.5% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de40c8",
   "metadata": {
    "id": "4707146a"
   },
   "source": [
    "## TreeClassifier evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815f241",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1098b17",
    "outputId": "5d603c41-6dce-4e79-c5ab-ca131919453a"
   },
   "outputs": [],
   "source": [
    "DTFinal = TreeClassifier(max_depth=7,criterion='gini')\n",
    "DTFinal.fit(Xtrain, Ytrain)\n",
    "Yguess = DTFinal.predict(Xtest)\n",
    "print('The testing accuracy of the TreeClassifier model is: ',round(accuracy_score(Ytest, Yguess),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81f82c",
   "metadata": {
    "id": "c2a1dd4c"
   },
   "source": [
    "By testing the classifier, we got a testing accuracy of 91% by using the hyperparameter <b>max_depth = 7</b> and <b>criterion = 'gini'</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78b2a7",
   "metadata": {},
   "source": [
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d1543",
   "metadata": {
    "id": "a2ef5135"
   },
   "source": [
    "# Regressors applications in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53fc9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "6ab00e30",
    "outputId": "83a2e4f9-e31c-41d3-f3f1-652d57ec74e7"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file using Pandas.\n",
    "alldata = pd.read_csv('sberbank.csv')\n",
    "alldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74b5dc",
   "metadata": {
    "id": "4c446394",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the timestamp string to an integer representing the year.\n",
    "def get_year(timestamp):\n",
    "    return int(timestamp[:4])\n",
    "alldata['year'] = alldata.timestamp.apply(get_year)\n",
    "\n",
    "# Select the 9 input columns and the output column.\n",
    "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
    "alldata = alldata[selected_columns]\n",
    "alldata = alldata.dropna()\n",
    "\n",
    "# Shuffle.\n",
    "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b9671",
   "metadata": {
    "id": "a8193c57"
   },
   "outputs": [],
   "source": [
    "# Separate the input and output columns.\n",
    "X = alldata_shuffled.drop('price_doc', axis=1)\n",
    "# For the output, we'll use the log of the sales price.\n",
    "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
    "\n",
    "# Split into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56aca9c",
   "metadata": {
    "id": "a916571c"
   },
   "source": [
    "# Dummy Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff921699",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "379cc1e9",
    "outputId": "5d068676-6e7a-4b77-ebc5-5c46a0c345e0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "m1 = DummyRegressor()\n",
    "cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8d177",
   "metadata": {},
   "source": [
    "By cross-validation the dummy classifier on the training data's different fold, we get a mean square error of 0.39 in predicting   the price of an apartment, given numerical information such as the number of rooms, the size of the apartment in square meters, the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d29ec1",
   "metadata": {
    "id": "f1f30172"
   },
   "source": [
    "# Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d6cd9",
   "metadata": {},
   "source": [
    "We firstly start by experimenting the Linear Regression of sklearn on our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e11764",
   "metadata": {
    "id": "e5254264"
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be40648",
   "metadata": {
    "id": "f6532015"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd5f25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "139c7fd1",
    "outputId": "d3791457-3a85-4941-b7d6-a3519427beda"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ligr = LinearRegression()\n",
    "cross_validate(ligr, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f833623",
   "metadata": {},
   "source": [
    "By cross-validation the Linear Regression on the training data's different folds, we get an average mean square error of 0.301 between the different folds of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84368d",
   "metadata": {
    "id": "15729609"
   },
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e0364",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63a8a741",
    "outputId": "565deb09-037d-40a1-9b8c-5d34d314ab6c"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rid = Ridge()\n",
    "cross_validate(rid, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3a1cd",
   "metadata": {},
   "source": [
    "As we can see, the Ridge model obtained an average cross-validation mean squared error  of 0.301 between the different folds of the training set, so we proceed by tuning the Ridge model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010efb2b",
   "metadata": {
    "id": "77772817"
   },
   "source": [
    "### tuning Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8285113",
   "metadata": {
    "id": "5ca1f810"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b261e2",
   "metadata": {
    "id": "a18f69a4"
   },
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "model = Ridge()\n",
    "grid = dict()\n",
    "grid['alpha'] = arange(0,50000,1000)\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "RidgeTuned = search.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c5f18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67759494",
    "outputId": "d0966cbd-a1a6-4977-f17a-a0ca3d299da3"
   },
   "outputs": [],
   "source": [
    "print(RidgeTuned.best_score_*-1)\n",
    "print(RidgeTuned.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c647c3",
   "metadata": {
    "id": "746f3e3f"
   },
   "source": [
    "By tuning Ridge model with L2 penalty lamda (alpha in the code) of different values in a sequence of values of [0,1000,...,50000], we found that Rigde model with the value lambda (alpha) equal to 49000 had mean squared error of 0.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560a776",
   "metadata": {
    "id": "5fe29e13"
   },
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b365e8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a4528f9",
    "outputId": "45dd671e-5658-4e2d-e7a5-7055ab15dd7a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "cross_validate(lasso, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cef4c2",
   "metadata": {},
   "source": [
    "In Lasso model, we also got an average (mean squared error) of 0.301 by cross validation between the different folds of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532adde7",
   "metadata": {
    "id": "ca75b7cb"
   },
   "source": [
    "#### Lasso tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a5f99",
   "metadata": {
    "id": "0c4cb6c2"
   },
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "lasso_alphas = arange(0, 0.1,10)\n",
    "lasso = Lasso()\n",
    "grid = dict()\n",
    "grid['alpha'] = lasso_alphas\n",
    "\n",
    "search = GridSearchCV(lasso, grid, scoring='neg_mean_squared_error',cv=cv, n_jobs=-1)\n",
    "lassoTuned = search.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e2f01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68bd78c9",
    "outputId": "9fc6551b-ecb6-4217-cd16-910659dc2223"
   },
   "outputs": [],
   "source": [
    "print(lassoTuned.best_score_*-1)\n",
    "print(lassoTuned.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ddd77",
   "metadata": {
    "id": "5aedfa85"
   },
   "source": [
    "By tuning the model using a gridsearch, the gridsearch found that the best L1 penalty lambda (alpha) = 0 which resulted in a MSE of 0.308"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b068f31",
   "metadata": {
    "id": "11437f83"
   },
   "source": [
    "# Tree regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0274109",
   "metadata": {
    "id": "a64b2ac8"
   },
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda39e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "351c68cb",
    "outputId": "de429c85-8a58-4e44-ca6a-3b6c55171fc3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "cross_validate(dtr, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f612585",
   "metadata": {},
   "source": [
    "As we can see, the Decision Tree Regressor obtained an average cross-validation mean squared error of 0.522 between the different folds of the training data, so we proceed by tuning the Decision Tree Regressor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2945283",
   "metadata": {
    "id": "a6f1f487"
   },
   "source": [
    "### Tuning decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc827ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "117ba4be",
    "outputId": "e8a8a1ec-4865-49aa-c464-a02c1a7f10cd"
   },
   "outputs": [],
   "source": [
    "### Tuning using sklearn gridsearch\n",
    "dTrgridTuned = DecisionTreeRegressor()\n",
    "parameter_space = {'max_depth': [3,5,10,40,70,100],\n",
    "                'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']\n",
    "                  }\n",
    "DtTuned = GridSearchCV(dTrgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "DtTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4e7e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64c32962",
    "outputId": "9517574d-2d0b-430f-e9eb-e98805f52840"
   },
   "outputs": [],
   "source": [
    "print(DtTuned.best_params_)\n",
    "print(DtTuned.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbc6b5",
   "metadata": {
    "id": "b9d12ecf"
   },
   "source": [
    "By tuning the Decision tree regressor model on the parameters<b> max_depth</b> and <b> criterion</b>, we found that the hyperparameters values that minimizes the model's MSE are <b> max_depth = 5</b> and <b> creterion = 'friedman_mse'</b> which resulted in a MSE value of 0.27 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30f5a0",
   "metadata": {
    "id": "5b9f0c2a"
   },
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9fbaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15a1c2d4",
    "outputId": "08b8fd03-0333-41c9-915c-9a42011e9dcc"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "cross_validate(regr, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1### Tuning decision tree regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256d0b4",
   "metadata": {},
   "source": [
    "In the Random Forest Regressor obtained an average cross-validation mean squared error of 0.266, so we then proceed by tuning the Random Forest Regressor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7d15e",
   "metadata": {
    "id": "e4f2ddc7"
   },
   "source": [
    "### Tuning Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92b463",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9968d49",
    "outputId": "3bca6378-74ed-4df2-cf33-e002ef06b7a9"
   },
   "outputs": [],
   "source": [
    "# ### Tuning using sklearn gridsearch\n",
    "RfRgridTuned = RandomForestRegressor()\n",
    "parameter_space = {'max_depth': [5,10,70],\n",
    "                  'n_estimators' : [100,500]}\n",
    "RFRTuned = GridSearchCV(RfRgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "RFRTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b1227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2f192b7",
    "outputId": "630821da-7f1e-4608-f065-95e2dd83d986"
   },
   "outputs": [],
   "source": [
    "print(RFRTuned.best_params_)\n",
    "print(RFRTuned.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9263681",
   "metadata": {
    "id": "6244497b"
   },
   "source": [
    "By tuning the Random forest regressor model on the parameters<b> max_depth</b> and <b>n_estimators</b> , we found that the hyperparameters values that minimizes the model's MSE are <b> max_depth = 10</b> and <b> n_estimators = 500</b> which resulted in a MSE value of 0.317 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6f885",
   "metadata": {
    "id": "60f77544"
   },
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d41a01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1cb4edc",
    "outputId": "543edb8d-56a1-400d-cd1b-5980791f2f49"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "cross_validate(gbr, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683c0d2",
   "metadata": {},
   "source": [
    "The Gradient Boosting Regressor obtained an average cross-validation mean squared error of 0.265, so we then proceed by tuing the Gradient Boosting Regressor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ad397",
   "metadata": {
    "id": "32721214"
   },
   "source": [
    "### Gradient boosting tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c1666",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac52f75f",
    "outputId": "9dd6615c-8da3-4444-9c5f-c02ab1930ba9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GBRgridTuned = GradientBoostingRegressor()\n",
    "parameter_space = {\n",
    "    # 'loss' : ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "    # 'n_estimators' : [ 2, 4, 8, 16, 32, 64, 100, 200,500]\n",
    "                'criterion': ['mse','friedman_mse'], #'mae','squared_error',  \n",
    "                  'alpha': [0.0001,0.001,0.01 ]}\n",
    "GBRTuned = GridSearchCV(GBRgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "GBRTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb4521",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2385d307",
    "outputId": "6207f79b-8ece-4087-9956-0a8c23df1091"
   },
   "outputs": [],
   "source": [
    "print(GBRTuned.best_params_)\n",
    "print(GBRTuned.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470309ba",
   "metadata": {},
   "source": [
    "By tuning the Gradient boosting on the parameters alpha and criterion, we found that the hyperparameters values that minimizes the model's MSE are \n",
    "<b>creterion = 'friedman_mse'</b> and <b>alpha = 0.001 </b>and which resulted in a MSE value of <b>0.32</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31e588",
   "metadata": {
    "id": "4e773839"
   },
   "source": [
    "### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454a030",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f767a68c",
    "outputId": "732a0173-d62f-4237-c53e-d7d00db1977e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor()\n",
    "cross_validate(mlp, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7058da71",
   "metadata": {},
   "source": [
    "The MLP Regressor obtained an average cross-validation mean squared error of 24, so we then proceed by tuing the MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf33977",
   "metadata": {
    "id": "e31c23fa"
   },
   "source": [
    "### Tuning MLP regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8b998",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4edd10a",
    "outputId": "60a62bc3-ca09-47b6-854e-585169b9eb63"
   },
   "outputs": [],
   "source": [
    "mlpRgridTuned = MLPRegressor()\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(150,),(1000,)],#,(3000,)\n",
    "    'activation': ['relu','logistic'],#'tanh', \n",
    "    # 'solver': ['lbfgs', 'sgd','adam'],\n",
    "    # 'alpha': [0.0001,0.001,0.01,0.1,1],\n",
    "    # 'learning_rate': ['constant','invscaling', 'adaptive']\n",
    "}\n",
    "mlpTuned = GridSearchCV(mlpRgridTuned, parameter_space, n_jobs=-1, cv=5)\n",
    "mlpTuned.fit(Xtrain, Ytrain) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a323f3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68bb2d32",
    "outputId": "ed9e4869-11bc-4168-bf8a-1ed1f1eecd4e"
   },
   "outputs": [],
   "source": [
    "print(mlpTuned.best_params_)\n",
    "print(mlpTuned.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcaebb",
   "metadata": {},
   "source": [
    "By tuning the MLP on the parameters alpha and criterion, we found that the hyperparameters values that minimizes the model's MSE are \n",
    "<b>activation = 'logistic'</b> and <b>hidden_layer_sizes = (1000,)</b>and which resulted in a MSE value of <b>0.022</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e722e",
   "metadata": {
    "id": "c11cb375"
   },
   "source": [
    "# Models evaluation and test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca928351",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwfpiZIWnxZF",
    "outputId": "18ffc30c-9a53-409f-8b3b-d02687928b21"
   },
   "outputs": [],
   "source": [
    "print(\"Regressors cross-validation mean squared error report: \")\n",
    "print('\\n')\n",
    "print(\"Dummy Regressor:\", cross_validate(m1, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1)\n",
    "print('\\n')\n",
    "print(\"Decision Tree Regressor:\", DtTuned.best_score_)\n",
    "print('\\n')\n",
    "print(\"Random Forest Regressor:\", cross_validate(regr, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1)\n",
    "print('\\n')\n",
    "print(\"Gradient Boosting Regressor:\", cross_validate(gbr, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1)\n",
    "print('\\n')\n",
    "print(\"Ridge Regressor:\", RidgeTuned.best_score_*-1)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Lasso Regressor:\",lassoTuned.best_score_*-1)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Linear Regression:\", cross_validate(ligr, Xtrain, Ytrain, scoring='neg_mean_squared_error')['test_score'].mean()*-1)\n",
    "\n",
    "print('\\n')\n",
    "print(\"MLP Regressor:\", mlpTuned.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1f05a",
   "metadata": {
    "id": "Ta9RCVjHraAQ"
   },
   "source": [
    "## Best performing models predictions of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71c503",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eQd7kmqrVhL",
    "outputId": "b7758081-f191-47b8-fc68-c325c0d91e1e"
   },
   "outputs": [],
   "source": [
    "# predictions using MLP NN\n",
    "# mlpTuned.fit(Xtrain,Ytrain)\n",
    "Yguess = mlpTuned.predict(Xtest)\n",
    "print('MLP Regressor: ')\n",
    "print( 'MSE: '+str(mean_squared_error(Ytest,Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0e51f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVBo-EQzr196",
    "outputId": "9039acde-2f2d-4eaa-952d-9186d076a6f7"
   },
   "outputs": [],
   "source": [
    "# predictions using RandomForest Regressor\n",
    "regr.fit(Xtrain,Ytrain)\n",
    "Yguess = regr.predict(Xtest)\n",
    "print('RandomForest Regressor: ')\n",
    "print( 'MSE: '+str(mean_squared_error(Ytest,Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a1a56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6SYASSVsv_m",
    "outputId": "3f8457f6-e811-4642-d90d-15416fb20d6f"
   },
   "outputs": [],
   "source": [
    "# predictions using Gradient Boosting Regressor\n",
    "gbr.fit(Xtrain,Ytrain)\n",
    "Yguess = gbr.predict(Xtest)\n",
    "print('Gradient Boosting Regressor: ')\n",
    "print( 'MSE: '+str(mean_squared_error(Ytest,Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935c75f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x67bAn1ZtIRN",
    "outputId": "c0d2d762-e19e-490c-ca9c-2dafc24a75cd"
   },
   "outputs": [],
   "source": [
    "# predictions using DecisionTree Regressor\n",
    "Yguess = DtTuned.predict(Xtest)\n",
    "print('Decision Tree Regressor: ')\n",
    "print( 'MSE: '+str(mean_squared_error(Ytest,Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842a906",
   "metadata": {},
   "source": [
    "By testing the best performing models in the cross validation phase with predicting the test set, the best performing model in predicting the testing set was the Gradient boosting regressor which predicted the testing set with a mean squared error of approximately 0.27 with a difference of +0.01 in the MSE than its cross-validation accuracy, the model was tuned using  gridsearch to find that the hyperparameters that minimizes the mse are <b>alpha= 0.001 and criterion = 'friedman_mse' </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df3959",
   "metadata": {
    "id": "86df3959"
   },
   "source": [
    "# Implementing a Decision Tree class for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21303a61",
   "metadata": {
    "id": "21303a61"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeLeaf:\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
    "    def predict(self, x):\n",
    "        return self.value\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_id = str(node_counter)\n",
    "        val_str = f'{self.value:.4g}' if isinstance(self.value, float) else str(self.value)\n",
    "        graph.node(node_id, val_str, style='filled')\n",
    "        return node_counter+1, node_id\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, DecisionTreeLeaf):\n",
    "            return self.value == other.value\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de035b",
   "metadata": {
    "id": "57de035b"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeBranch:\n",
    "\n",
    "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.low_subtree = low_subtree\n",
    "        self.high_subtree = high_subtree\n",
    "\n",
    "    # For a branch node, we compute the prediction by first considering the feature, and then \n",
    "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
    "    # than the threshold.\n",
    "    def predict(self, x):\n",
    "        if x[self.feature] <= self.threshold:\n",
    "            return self.low_subtree.predict(x)\n",
    "        else:\n",
    "            return self.high_subtree.predict(x)\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_counter, low_id = self.low_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_counter, high_id = self.high_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_id = str(node_counter)\n",
    "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
    "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
    "        graph.node(node_id, lbl, shape='box', fillcolor='yellow', style='filled, rounded')\n",
    "        graph.edge(node_id, low_id, 'False')\n",
    "        graph.edge(node_id, high_id, 'True')\n",
    "        return node_counter+1, node_id\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df0c02",
   "metadata": {
    "id": "97df0c02",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DecisionTree(ABC, BaseEstimator):\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
    "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
    "    # called make_tree (see below).\n",
    "    def fit(self, X, Y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.names = X.columns\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, list):\n",
    "            self.names = None\n",
    "            X = np.array(X)\n",
    "        else:\n",
    "            self.names = None\n",
    "        Y = np.array(Y)        \n",
    "        self.root = self.make_tree(X, Y, self.max_depth)\n",
    "        \n",
    "    def draw_tree(self):\n",
    "        graph = Digraph()\n",
    "        self.root.draw_tree(graph, 0, self.names)\n",
    "        return graph\n",
    "    \n",
    "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
    "    # for a set of instances.\n",
    "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        return [self.predict_one(x) for x in X]\n",
    "\n",
    "    # Predicting the output for one instance.\n",
    "    def predict_one(self, x):\n",
    "        return self.root.predict(x)        \n",
    "\n",
    "    # This is the recursive training \n",
    "    def make_tree(self, X, Y, max_depth):\n",
    "\n",
    "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
    "        # For classifiers, this will be the most common value in Y.\n",
    "        default_value = self.get_default_value(Y)\n",
    "\n",
    "        # First the two base cases in the recursion: is the training set completely\n",
    "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
    "\n",
    "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
    "        if max_depth == 0:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # If all the instances in the remaining training set have the same output value,\n",
    "        # return a leaf with this value.\n",
    "        if self.is_homogeneous(Y):\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
    "        # we use one of the classification or regression criteria.\n",
    "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
    "        n_features = X.shape[1]\n",
    "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
    "        # the threshold or not\n",
    "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
    "\n",
    "        # Build the subtrees using a recursive call. Each subtree is associated\n",
    "        # with a value of the feature.\n",
    "        low_subtree = self.make_tree(X_low, Y_low, max_depth-1)\n",
    "        high_subtree = self.make_tree(X_high, Y_high, max_depth-1)\n",
    "\n",
    "        if low_subtree == high_subtree:\n",
    "            return low_subtree\n",
    "\n",
    "        # Return a decision tree branch containing the result.\n",
    "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
    "    \n",
    "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
    "    # and a threshold.\n",
    "    def split_by_feature(self, X, Y, feature, threshold):\n",
    "        low = X[:,feature] <= threshold\n",
    "        high = ~low\n",
    "        return X[low], X[high], Y[low], Y[high]\n",
    "    \n",
    "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_default_value(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_homogeneous(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def best_split(self, X, Y, feature):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a3b04",
   "metadata": {
    "id": "772a3b04"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TreeRegressor(DecisionTree, RegressorMixin):\n",
    "\n",
    "    def __init__(self, max_depth=10, threshold = 0.2,  criterion='var_red'):\n",
    "        super().__init__(max_depth)\n",
    "        self.criterion = criterion\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # For decision tree classifiers, there are some different ways to measure\n",
    "        # the homogeneity of subsets.\n",
    "        if self.criterion == 'var_red':\n",
    "            self.criterion_function = variance_reduction_scorer\n",
    "        else:\n",
    "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
    "        super().fit(X, Y)\n",
    "        self.classes_ = sorted(set(Y))\n",
    "\n",
    "    # Select a default value that is going to be used if we decide to make a leaf.\n",
    "    # We will select the most common value.\n",
    "    def get_default_value(self, Y):\n",
    "        return np.mean(Y)\n",
    "    \n",
    "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
    "    # this means that all output values are identical.\n",
    "    # We assume that we called get_default_value just before, so that we can access\n",
    "    # the class_distribution attribute. If the class distribution contains just one item,\n",
    "    # this means that the set is homogeneous.\n",
    "    def is_homogeneous(self, Y):\n",
    "        return np.var(Y) <= self.threshold #threshold (set to be 0.2 through trial and error to minimize the mse)\n",
    "        \n",
    "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
    "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
    "    # In the end, we return a triple consisting of\n",
    "    # - the best score we found, according to the criterion we're using\n",
    "    # - the id of the feature\n",
    "    # - the threshold for the best split\n",
    "    def best_split(self, X, Y, feature):\n",
    "\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        sorted_indices = np.argsort(X[:, feature])        \n",
    "        X_sorted = list(X[sorted_indices, feature])\n",
    "        Y_sorted = list(Y[sorted_indices])\n",
    "\n",
    "        n = len(Y)\n",
    "        \n",
    "        \n",
    "        # declaring the 2 subsets of the feature as empty list and the full list before entering the loop\n",
    "        low_distr = []\n",
    "        high_distr = Y_sorted\n",
    "        \n",
    "        # Keep track of the best result we've seen so far.\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "\n",
    "\n",
    "        # Go through all the positions (excluding the last position).\n",
    "        for i in range(0, n-1):\n",
    "\n",
    "            # Input and output at the current position.\n",
    "            x_i = X_sorted[i]\n",
    "            y_i = Y_sorted[i]\n",
    "\n",
    "            \n",
    "            # Update the frequency tables.\n",
    "#             low_distr[y_i] += 1\n",
    "#             high_distr[y_i] -= 1\n",
    "            \n",
    "            #low_distr subset is the output variable items before the current instance, and \n",
    "            #high_distr subset is the output variable items from and after the current instance, and \n",
    "            low_distr =  Y_sorted[0:i]    \n",
    "            high_distr = Y_sorted[i:]\n",
    "\n",
    "            # If the input is equal to the input at the next position, we will\n",
    "            # not consider a split here.\n",
    "            #x_next = XY[i+1][0]\n",
    "            x_next = X_sorted[i+1]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "                \n",
    "\n",
    "            # Compute the homogeneity criterion for a split at this position.\n",
    "            score = self.criterion_function(i+1, low_distr, n-i-1, high_distr)\n",
    "\n",
    "            # If this is the best split, remember it.\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "        # If we didn't find any split (meaning that all inputs are identical), return\n",
    "        # a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "        # Otherwise, return the best split we found and its score.\n",
    "        split_point = 0.5*(X_sorted[max_i] + X_sorted[max_i+1])\n",
    "        return max_score, feature, split_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc17d70",
   "metadata": {
    "id": "bcc17d70"
   },
   "outputs": [],
   "source": [
    "def variance_reduction_scorer(n_low, low_distr, n_high, high_distr): \n",
    "    return np.var(low_distr + high_distr)-n_low/(n_low+n_high)*np.var(high_distr)-n_high/(n_low+n_high)*np.var(low_distr)\n",
    "\n",
    "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
    "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
    "    return maj_sum_low + maj_sum_high\n",
    "    \n",
    "def entropy(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return -sum(p*np.log2(p) if p > 0 else 0 for p in ps)\n",
    "\n",
    "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*entropy(low_distr)+n_high*entropy(high_distr))/(n_low+n_high)\n",
    "\n",
    "def gini_impurity(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return 1-sum(p**2 for p in ps)\n",
    "    \n",
    "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*gini_impurity(low_distr)+n_high*gini_impurity(high_distr))/(n_low+n_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89eb64",
   "metadata": {},
   "source": [
    "1. To define the threshold, instead of setting it to a constant value that might not maximize the model's accuracy, we added a new parameter <b> threshold</b> to the init function of the model with a default value threshold = 0.2, so we can add it to the gridsearch later on, so we can get the best threshold for the homogenity of the set by the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fab414",
   "metadata": {},
   "source": [
    "2. To select a default value that is going to be used if we decide to make a leaf, we select the mean value of the output variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad65820",
   "metadata": {},
   "source": [
    "3. The split that leads to the highest homogenity is selected by calculating the variance reduction function of the 2 subsets of the output variable at each observation and taking the split with the highest homogenity score, to do that, we declared the function <i>variance_reduction_scorer(n_low, low_distr, n_high, high_distr)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba6b4e",
   "metadata": {
    "id": "29ba6b4e"
   },
   "outputs": [],
   "source": [
    "dtr = TreeRegressor(max_depth=5)\n",
    "dtr.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467c4b0",
   "metadata": {
    "id": "4467c4b0"
   },
   "outputs": [],
   "source": [
    "Yguess = dtr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690223e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b690223e",
    "outputId": "6e83ccf5-5fa0-4d18-af73-a42038bf750f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "  \n",
    "mean_squared_error(Ytest,Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0c900",
   "metadata": {},
   "source": [
    "we then use the class Tree Regressor to predict the testing set, and we get a mean squared error of 0.4 on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5556167",
   "metadata": {
    "id": "e5556167"
   },
   "source": [
    "## Tuning TressRegressor hyperparameters with gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8b536",
   "metadata": {
    "id": "95a8b536"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe06899",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fe06899",
    "outputId": "0e02fb4b-685c-4fec-fceb-74558fa93601",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # define grid\n",
    "# model = TreeRegressor()\n",
    "# grid = dict()\n",
    "# grid['max_depth'] = [6,12]\n",
    "# grid['threshold'] = [0.1,0.2,0.3]\n",
    "\n",
    "# # define search\n",
    "# search = GridSearchCV(model, grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# # perform the search\n",
    "# results = search.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce76200",
   "metadata": {
    "id": "2ce76200"
   },
   "outputs": [],
   "source": [
    "# results.best_params_\n",
    "# results.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737aa16b",
   "metadata": {
    "id": "737aa16b"
   },
   "source": [
    "### TreeRegressorer model max_depth hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7274617",
   "metadata": {
    "id": "d7274617",
    "scrolled": true
   },
   "source": [
    "### Tuning using the method using above (max_depth plot vs acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7d501",
   "metadata": {
    "id": "98b7d501",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 15, 15, endpoint=True)\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    dt = TreeRegressor(max_depth=max_depth)\n",
    "    dt.fit(Xtrain, Ytrain)\n",
    "    Ypred = dt.predict(Xtest)\n",
    "    mseTemp = mean_squared_error(Ytest, Ypred)\n",
    "    test_results.append(mseTemp)\n",
    "    \n",
    "plt.plot(max_depths, test_results)\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a95cfe",
   "metadata": {},
   "source": [
    "By tuning the class TreeRegressor through a graph, we get that the max_depth accuracy that minimizes the mse is max_depth = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc465de6",
   "metadata": {
    "id": "fc465de6"
   },
   "outputs": [],
   "source": [
    "def make_some_data(n):\n",
    "    x = np.random.uniform(-5, 5, size=n)\n",
    "    Y = (x > 1) + 0.1*np.random.normal(size=n)\n",
    "    X = x.reshape(n, 1) # X needs to be a 2-dimensional matrix\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014920e",
   "metadata": {
    "id": "2014920e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataTemp = make_some_data(100)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(dataTemp[0], dataTemp[1], cmap='tab10');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbae9b",
   "metadata": {},
   "source": [
    "We generate a 100 data point randomly using the data generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04b1ab",
   "metadata": {},
   "source": [
    "from the plot above, the data looks that it can be best described through a decision tree classifier as the data is very well splitted into two groups in the 2D space, a decision tree classifier can  classify the data points into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4594bd",
   "metadata": {
    "id": "1a4594bd"
   },
   "outputs": [],
   "source": [
    "dataTemp = make_some_data(1000)\n",
    "XtrainTemp, XtestTemp, YtrainTemp, YtestTemp = train_test_split(dataTemp[0], dataTemp[1], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20feacc8",
   "metadata": {
    "id": "20feacc8"
   },
   "outputs": [],
   "source": [
    "dtrTemp = TreeRegressor(max_depth=5)\n",
    "dtrTemp.fit(XtrainTemp, YtrainTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0e9d4",
   "metadata": {
    "id": "9db0e9d4"
   },
   "outputs": [],
   "source": [
    "Yguess = dtrTemp.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f391f",
   "metadata": {
    "id": "3d4f391f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "  \n",
    "mean_squared_error(Ytest,Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613d3b8",
   "metadata": {},
   "source": [
    "By training and testing the TreeRegressor class on the data above after splitting it into test and train data, we get a mean square error of 233 on the testing set when the max_depth is set to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c49495",
   "metadata": {
    "id": "27c49495"
   },
   "outputs": [],
   "source": [
    "dtrTemp.draw_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dce448",
   "metadata": {},
   "source": [
    "After drawing the tree, we can see that the result doesn' make sense as the root node started by value = -4.926 and the data was generated on an inteval on [-5,5], so a max_depth 5 as shown in the graph doesn't predict well the values of the data due to the severe deviation of the data values and a small depth such as 5 can't capture all the data points and predict them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e1ae0",
   "metadata": {
    "id": "f98e1ae0"
   },
   "outputs": [],
   "source": [
    "dtr = TreeRegressor(max_depth=5)\n",
    "dtr.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea195a",
   "metadata": {
    "id": "2bea195a"
   },
   "outputs": [],
   "source": [
    "Yguess = dtr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44518668",
   "metadata": {
    "id": "44518668"
   },
   "outputs": [],
   "source": [
    "mean_squared_error(Ytest,Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c9026",
   "metadata": {},
   "source": [
    "The TreeRegressor class predicted the price of the apartments with a mean squared error of 0.4 on the testing set with <b>max_depth hyperparameter set to 5</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4f265",
   "metadata": {
    "id": "1ad4f265",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 12, 12, endpoint=True)\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    dt = TreeRegressor(max_depth=max_depth)\n",
    "    dt.fit(Xtrain, Ytrain)\n",
    "    YpredTrain = dt.predict(Xtrain)\n",
    "    mseTrain = mean_squared_error(Ytrain, YpredTrain)\n",
    "    Ypredtest = dt.predict(Xtest)\n",
    "    mseTest = mean_squared_error(Ytest, Ypredtest)\n",
    "    train_results.append(mseTrain)\n",
    "    test_results.append(mseTest)\n",
    "\n",
    "    \n",
    "line1, = plt.plot(max_depths,train_results, '--bo', 'b', label=\"Train MSE\")\n",
    "line2, = plt.plot(max_depths,test_results, '--bo',  'r', label=\"Test MSE\")\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1672705",
   "metadata": {},
   "source": [
    "From the graph above, we can see that when the model is used to predict the training data that it was trained on, it achieved a minimum MSE of 0.38 at tree_depth = 12 (the blue line), while it achieved a testing MSE of 0.395 at tree_depth = 12 when the model is used to predict an unseen testing data (the red line), so we can conclude from the graph above that since the trends of both curves are almost the same when increasing and decreasing the tree_depth while maintaining almost a constant very low difference in the MSE between the training and testing curves, that the model is not overfitting and it predicted the unseen testing data with a proper efficiency compared to its efficiency in predicting the previously seen training data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a2ef5135",
    "a916571c",
    "f1f30172",
    "e5254264",
    "15729609",
    "77772817",
    "5fe29e13",
    "ca75b7cb",
    "11437f83",
    "a64b2ac8",
    "a6f1f487",
    "5b9f0c2a",
    "e4f2ddc7",
    "8b2271ea",
    "60f77544",
    "32721214",
    "4e773839",
    "e31c23fa",
    "c11cb375",
    "86df3959",
    "a1a1943e",
    "e5556167",
    "737aa16b",
    "d7274617",
    "b20dff78",
    "90a0fbd1",
    "6024f118",
    "f817912c"
   ],
   "name": "Assignment1_task12.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
